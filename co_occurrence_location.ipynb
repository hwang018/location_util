{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Location related modules:\n",
    "\n",
    "    This module contains a few applications for location related use cases.\n",
    "    Since location data sources are often huge, this module is designed to work on spark dataframe.\n",
    "    \n",
    "    Including use cases:\n",
    "    \n",
    "    1. Approximate lat long points into bins using geohash.\n",
    "    2. Find co-location based on geohash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import findspark\n",
    "#spark path using default value\n",
    "findspark.init()\n",
    "import pyspark\n",
    "import pyarrow\n",
    "from pyspark.sql import SQLContext\n",
    "    \n",
    "from importlib import reload\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession,SQLContext\n",
    "from pyspark.sql.functions import collect_list,collect_set,struct,col,substring,lit,udf,concat\n",
    "from pyspark.sql.functions import unix_timestamp,from_unixtime,to_timestamp,to_date\n",
    "from pyspark.sql.functions import year,month,dayofmonth,hour,minute\n",
    "\n",
    "import geohash as gh\n",
    "### Note: track_id is the person's id.\n",
    "\n",
    "def initialize_spark(app_name='location'):\n",
    "    conf = pyspark.SparkConf()\\\n",
    "        .setAppName(app_name)\\\n",
    "        .setMaster('local')\\\n",
    "        .set('spark.driver.memory', '8g')\\\n",
    "        .set('spark.executor.memory', '8g')\\\n",
    "        .set('spark.executor.instances', 4)\\\n",
    "        .set('spark.executor.cores', 4)\\\n",
    "        .set('spark.driver.maxResultSize', '8g')\\\n",
    "        .set('spark.sql.shuffle.partitions', 100)\\\n",
    "        .set('spark.default.parallelism', 200)\\\n",
    "        .set('spark.sql.broadcastTimeout', 36000)\\\n",
    "        .set('spark.kryoserializer.buffer.max', '1024m')\\\n",
    "        .set('spark.sql.execution.arrow.enabled', 'false')\\\n",
    "        .set('spark.dynamicAllocation.enabled', \"False\")\\\n",
    "        .set('spark.port.maxRetries',30) \n",
    "\n",
    "    sc = pyspark.SparkContext.getOrCreate(conf)\n",
    "    spark = pyspark.sql.SparkSession(sc)\n",
    "    sqlContext = SQLContext.getOrCreate(sc)    \n",
    "    return sc,spark,sqlContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc,spark,sqlContext = initialize_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas to spark df, loading data\n",
    "def equivalent_type(f):\n",
    "    '''\n",
    "    add more spark sql types like bigint ...\n",
    "    '''\n",
    "    if f == 'datetime64[ns]': return DateType()\n",
    "    elif f == 'int64': return LongType()\n",
    "    elif f == 'int32': return IntegerType()\n",
    "    elif f == 'float64': return FloatType()\n",
    "    else: return StringType()\n",
    "\n",
    "def define_structure(string, format_type):\n",
    "    try: typo = equivalent_type(format_type)\n",
    "    except: typo = StringType()\n",
    "    return StructField(string, typo)\n",
    "\n",
    "def pandas_to_spark(sqlcontext,pandas_df):\n",
    "    columns = list(pandas_df.columns)\n",
    "    types = list(pandas_df.dtypes)\n",
    "    struct_list = []\n",
    "    for column, typo in zip(columns, types): \n",
    "        struct_list.append(define_structure(column, typo))\n",
    "    p_schema = StructType(struct_list)\n",
    "    return sqlcontext.createDataFrame(pandas_df, p_schema)\n",
    "\n",
    "def load_basic_location_data(path,sqlContext):\n",
    "    '''\n",
    "    get location raw data given a period\n",
    "    inputs:\n",
    "    * path: path to csv location data\n",
    "    * sqlContext\n",
    "    output:\n",
    "    * location_spark_df: spark df, containing location info\n",
    "    '''\n",
    "    location_pandas_df = pd.read_csv(path)\n",
    "    \n",
    "    location_spark_df = pandas_to_spark(sqlContext,location_pandas_df)\n",
    "    \n",
    "    location_spark_df = location_spark_df.withColumn(\"points\",struct(col(\"latitude\").cast(\"Float\"),col(\"longitude\").cast(\"Float\")))\n",
    "    # convert into spark df's time dtype\n",
    "    location_spark_df = location_spark_df.withColumn(\"datetime\", to_timestamp(location_spark_df.time, 'yyyy-MM-dd HH:mm:ss'))\n",
    "    \n",
    "    return location_spark_df.select(['track_id','points','datetime'])\n",
    "\n",
    "@udf(\"string\")\n",
    "def geohash_w_time(struct_input):\n",
    "    # using geohash to put points to bins\n",
    "    encoded = gh.encode(struct_input[0],struct_input[1],precision=6)\n",
    "    return encoded\n",
    "\n",
    "def get_hashed_points_w_time(path,sqlContext):\n",
    "    '''\n",
    "    use geohash to hash location points\n",
    "    stamp with time and create time-location struct\n",
    "    '''\n",
    "    basic_location_df = load_basic_location_data(path,sqlContext)\n",
    "    basic_location_df = basic_location_df.withColumn('geohashed_point',geohash_w_time('points'))\n",
    "    \n",
    "    basic_location_df = basic_location_df.withColumn(\"easytime\", concat(concat(year(col('datetime')),\\\n",
    "                                            lit(\"-\"),month(col('datetime')),lit(\"-\"),\\\n",
    "                                           dayofmonth(col('datetime')),\\\n",
    "                                           lit(\":\"),hour(col('datetime')))))\n",
    "    \n",
    "    basic_location_df = basic_location_df.withColumn(\"points_w_time\",struct(col(\"easytime\"),col(\"geohashed_point\")))\n",
    "\n",
    "    keep_cols = ['track_id','easytime','points_w_time','geohashed_point']\n",
    "    \n",
    "    return basic_location_df.select(*keep_cols)\n",
    "\n",
    "def find_co_location(sdf):\n",
    "    sdf_agg = sdf.groupBy(\"easytime\",\"geohashed_point\").agg(collect_set(col(\"track_id\")).alias(\"co_occurrence\"))\n",
    "    return sdf_agg\n",
    "\n",
    "@udf(\"Boolean\")\n",
    "def has_co_occurrence(set_input):\n",
    "    bool_val = len(set_input) > 1\n",
    "    return bool_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'datasets/go_track_trackspoints.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = get_hashed_points_w_time(path,sqlContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------------------+---------------+\n",
      "|track_id|   easytime|       points_w_time|geohashed_point|\n",
      "+--------+-----------+--------------------+---------------+\n",
      "|       1|2014-9-13:7|[2014-9-13:7, 7nj...|         7nj9u8|\n",
      "|       1|2014-9-13:7|[2014-9-13:7, 7nj...|         7nj9u8|\n",
      "|       1|2014-9-13:7|[2014-9-13:7, 7nj...|         7nj9u8|\n",
      "|       1|2014-9-13:7|[2014-9-13:7, 7nj...|         7nj9u8|\n",
      "|       1|2014-9-13:7|[2014-9-13:7, 7nj...|         7nj9u8|\n",
      "|       1|2014-9-13:7|[2014-9-13:7, 7nj...|         7nj9u8|\n",
      "|       1|2014-9-13:7|[2014-9-13:7, 7nj...|         7nj9u8|\n",
      "|       1|2014-9-13:7|[2014-9-13:7, 7nj...|         7nj9u8|\n",
      "|       1|2014-9-13:7|[2014-9-13:7, 7nj...|         7nj9u8|\n",
      "|       1|2014-9-13:7|[2014-9-13:7, 7nj...|         7nj9u8|\n",
      "|       1|2014-9-13:7|[2014-9-13:7, 7nj...|         7nj9u8|\n",
      "|       1|2014-9-13:7|[2014-9-13:7, 7nj...|         7nj9u8|\n",
      "|       1|2014-9-13:7|[2014-9-13:7, 7nj...|         7nj9u8|\n",
      "|       1|2014-9-13:7|[2014-9-13:7, 7nj...|         7nj9u8|\n",
      "|       1|2014-9-13:7|[2014-9-13:7, 7nj...|         7nj9u8|\n",
      "|       1|2014-9-13:7|[2014-9-13:7, 7nj...|         7nj9u8|\n",
      "|       1|2014-9-13:7|[2014-9-13:7, 7nj...|         7nj9u8|\n",
      "|       1|2014-9-13:7|[2014-9-13:7, 7nj...|         7nj9u8|\n",
      "|       1|2014-9-13:7|[2014-9-13:7, 7nj...|         7nj9u8|\n",
      "|       1|2014-9-13:7|[2014-9-13:7, 7nj...|         7nj9u8|\n",
      "+--------+-----------+--------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf2 = find_co_location(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+-------------+\n",
      "|     easytime|geohashed_point|co_occurrence|\n",
      "+-------------+---------------+-------------+\n",
      "|2014-10-23:18|         7nj9ut|         [28]|\n",
      "| 2014-11-4:19|         7njv59|     [30, 31]|\n",
      "|2014-11-28:12|         7nj9u1|         [39]|\n",
      "|  2015-2-12:3|         7nj9uz|        [131]|\n",
      "|   2015-3-2:3|         7nj9v5|        [153]|\n",
      "|  2015-5-19:6|         7nj9uk|      [37960]|\n",
      "|  2015-5-19:4|         7nj9gy|      [37962]|\n",
      "| 2015-5-29:10|         7nj9sb|      [38001]|\n",
      "| 2015-5-29:12|         7nj9km|      [38002]|\n",
      "|2015-11-23:12|         7nj9u0|      [38080]|\n",
      "| 2014-9-30:10|         7nj9uh|         [13]|\n",
      "| 2014-11-4:19|         7njutj|     [30, 31]|\n",
      "| 2014-11-4:19|         7njucq|     [30, 31]|\n",
      "| 2014-12-6:10|         7nj9ux|     [48, 47]|\n",
      "|  2015-2-19:7|         7nj9sq|   [140, 141]|\n",
      "|  2015-2-23:9|         7nj9t4|        [149]|\n",
      "| 2015-5-29:11|         7nj9uf|      [38002]|\n",
      "| 2015-5-29:12|         7nj9kw|      [38002]|\n",
      "|   2015-6-3:5|         7nj9ge|      [38013]|\n",
      "|2015-11-21:13|         7nj9sx|      [38074]|\n",
      "+-------------+---------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf3 = basic_location_df = sdf2.withColumn('result',has_co_occurrence('co_occurrence'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+-------------+------+\n",
      "|     easytime|geohashed_point|co_occurrence|result|\n",
      "+-------------+---------------+-------------+------+\n",
      "|2014-10-23:18|         7nj9ut|         [28]| false|\n",
      "| 2014-11-4:19|         7njv59|     [30, 31]|  true|\n",
      "|2014-11-28:12|         7nj9u1|         [39]| false|\n",
      "|  2015-2-12:3|         7nj9uz|        [131]| false|\n",
      "|   2015-3-2:3|         7nj9v5|        [153]| false|\n",
      "|  2015-5-19:6|         7nj9uk|      [37960]| false|\n",
      "|  2015-5-19:4|         7nj9gy|      [37962]| false|\n",
      "| 2015-5-29:10|         7nj9sb|      [38001]| false|\n",
      "| 2015-5-29:12|         7nj9km|      [38002]| false|\n",
      "|2015-11-23:12|         7nj9u0|      [38080]| false|\n",
      "| 2014-9-30:10|         7nj9uh|         [13]| false|\n",
      "| 2014-11-4:19|         7njutj|     [30, 31]|  true|\n",
      "| 2014-11-4:19|         7njucq|     [30, 31]|  true|\n",
      "| 2014-12-6:10|         7nj9ux|     [48, 47]|  true|\n",
      "|  2015-2-19:7|         7nj9sq|   [140, 141]|  true|\n",
      "|  2015-2-23:9|         7nj9t4|        [149]| false|\n",
      "| 2015-5-29:11|         7nj9uf|      [38002]| false|\n",
      "| 2015-5-29:12|         7nj9kw|      [38002]| false|\n",
      "|   2015-6-3:5|         7nj9ge|      [38013]| false|\n",
      "|2015-11-21:13|         7nj9sx|      [38074]| false|\n",
      "+-------------+---------------+-------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "266"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf3.filter(sdf3.result==True).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
