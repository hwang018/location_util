{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Location related modules:\n",
    "\n",
    "    This module contains a few applications for location related use cases.\n",
    "    Since location data sources are often huge, this module is designed to work on spark dataframe.\n",
    "    \n",
    "    Including use cases:\n",
    "    \n",
    "    1. Approximate lat long points into bins using geohash.\n",
    "    2. Find co-location based on geohash.\n",
    "    3. Find stationary location points, to infer special location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import findspark\n",
    "#spark path using default value\n",
    "findspark.init()\n",
    "\n",
    "\n",
    "import pyspark\n",
    "import pyarrow\n",
    "from pyspark.sql import SQLContext\n",
    "    \n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import when, lit\n",
    "from distutils.version import LooseVersion\n",
    "from importlib import reload\n",
    "import pyspark.sql.functions as func\n",
    "import pyspark.sql.types as typ\n",
    "\n",
    "import dateutil\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from dateutil.parser import parse\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col, countDistinct, when, row_number\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession,SQLContext\n",
    "\n",
    "import geohash as gh\n",
    "from collections import Counter\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import collect_list,struct,col,substring,lit,udf\n",
    "\n",
    "\n",
    "pd.options.display.max_columns=None\n",
    "pd.options.display.max_rows=None\n",
    "\n",
    "def initialize_spark(app_name='location'):\n",
    "    \n",
    "    \n",
    "    #broadcastTimeout is purposedly set to be large due to development on single machine\n",
    "    conf = pyspark.SparkConf()\\\n",
    "        .setAppName(app_name)\\\n",
    "        .setMaster('local')\\\n",
    "        .set('spark.driver.memory', '8g')\\\n",
    "        .set('spark.executor.memory', '8g')\\\n",
    "        .set('spark.executor.instances', 4)\\\n",
    "        .set('spark.executor.cores', 4)\\\n",
    "        .set('spark.driver.maxResultSize', '8g')\\\n",
    "        .set('spark.sql.shuffle.partitions', 100)\\\n",
    "        .set('spark.default.parallelism', 200)\\\n",
    "        .set('spark.sql.broadcastTimeout', 36000)\\\n",
    "        .set('spark.kryoserializer.buffer.max', '1024m')\\\n",
    "        .set('spark.sql.execution.arrow.enabled', 'false')\\\n",
    "        .set('spark.dynamicAllocation.enabled', \"False\")\\\n",
    "        .set('spark.port.maxRetries',30) \n",
    "\n",
    "    sc = pyspark.SparkContext.getOrCreate(conf)\n",
    "    spark = pyspark.sql.SparkSession(sc)\n",
    "    sqlContext = SQLContext.getOrCreate(sc)    \n",
    "    return sc,spark,sqlContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc,spark,sqlContext = initialize_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equivalent_type(f):\n",
    "    '''\n",
    "    add more spark sql types like bigint ...\n",
    "    '''\n",
    "    if f == 'datetime64[ns]': return DateType()\n",
    "    elif f == 'int64': return LongType()\n",
    "    elif f == 'int32': return IntegerType()\n",
    "    elif f == 'float64': return FloatType()\n",
    "    else: return StringType()\n",
    "\n",
    "def define_structure(string, format_type):\n",
    "    try: typo = equivalent_type(format_type)\n",
    "    except: typo = StringType()\n",
    "    return StructField(string, typo)\n",
    "\n",
    "def pandas_to_spark(sqlcontext,pandas_df):\n",
    "    columns = list(pandas_df.columns)\n",
    "    types = list(pandas_df.dtypes)\n",
    "    struct_list = []\n",
    "    for column, typo in zip(columns, types): \n",
    "        struct_list.append(define_structure(column, typo))\n",
    "    p_schema = StructType(struct_list)\n",
    "    return sqlcontext.createDataFrame(pandas_df, p_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>track_id</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-10.939341</td>\n",
       "      <td>-37.062742</td>\n",
       "      <td>1</td>\n",
       "      <td>2014-09-13 07:24:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>-10.939341</td>\n",
       "      <td>-37.062742</td>\n",
       "      <td>1</td>\n",
       "      <td>2014-09-13 07:24:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>-10.939324</td>\n",
       "      <td>-37.062765</td>\n",
       "      <td>1</td>\n",
       "      <td>2014-09-13 07:24:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>-10.939211</td>\n",
       "      <td>-37.062843</td>\n",
       "      <td>1</td>\n",
       "      <td>2014-09-13 07:24:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>-10.938939</td>\n",
       "      <td>-37.062879</td>\n",
       "      <td>1</td>\n",
       "      <td>2014-09-13 07:24:53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id   latitude  longitude  track_id                 time\n",
       "0   1 -10.939341 -37.062742         1  2014-09-13 07:24:32\n",
       "1   2 -10.939341 -37.062742         1  2014-09-13 07:24:37\n",
       "2   3 -10.939324 -37.062765         1  2014-09-13 07:24:42\n",
       "3   4 -10.939211 -37.062843         1  2014-09-13 07:24:47\n",
       "4   5 -10.938939 -37.062879         1  2014-09-13 07:24:53"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "location_pandas_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: track_id is the person's id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_spark_df = pandas_to_spark(sqlContext,location_pandas_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+----------+--------+-------------------+\n",
      "| id|   latitude| longitude|track_id|               time|\n",
      "+---+-----------+----------+--------+-------------------+\n",
      "|  1| -10.939342| -37.06274|       1|2014-09-13 07:24:32|\n",
      "|  2| -10.939342| -37.06274|       1|2014-09-13 07:24:37|\n",
      "|  3| -10.939324|-37.062763|       1|2014-09-13 07:24:42|\n",
      "|  4| -10.939211|-37.062843|       1|2014-09-13 07:24:47|\n",
      "|  5| -10.938939|-37.062878|       1|2014-09-13 07:24:53|\n",
      "|  6| -10.938543| -37.06284|       1|2014-09-13 07:24:59|\n",
      "|  7| -10.938346|-37.062588|       1|2014-09-13 07:25:04|\n",
      "|  8| -10.938449|   -37.062|       1|2014-09-13 07:25:10|\n",
      "|  9| -10.938666|  -37.0615|       1|2014-09-13 07:25:15|\n",
      "| 10| -10.938986|-37.060818|       1|2014-09-13 07:25:21|\n",
      "| 11|-10.9393425|-37.060085|       1|2014-09-13 07:25:27|\n",
      "| 12| -10.939641|  -37.0595|       1|2014-09-13 07:25:32|\n",
      "| 13|-10.9398575| -37.05912|       1|2014-09-13 07:25:38|\n",
      "| 14| -10.940077|-37.058727|       1|2014-09-13 07:25:43|\n",
      "| 15| -10.940389|-37.058292|       1|2014-09-13 07:25:49|\n",
      "| 16| -10.940746| -37.05799|       1|2014-09-13 07:25:54|\n",
      "| 17| -10.941179| -37.05764|       1|2014-09-13 07:26:00|\n",
      "| 18|  -10.94148|-37.057392|       1|2014-09-13 07:26:06|\n",
      "| 19| -10.941532|-37.057346|       1|2014-09-13 07:26:12|\n",
      "| 20| -10.941532|-37.057346|       1|2014-09-13 07:26:18|\n",
      "+---+-----------+----------+--------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "location_spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'datasets/go_track_trackspoints.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import unix_timestamp, from_unixtime, to_timestamp, to_date\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def load_basic_location_data(path,sqlContext):\n",
    "    '''\n",
    "    get location raw data given a period\n",
    "    inputs:\n",
    "    * path: path to csv location data\n",
    "    * sqlContext\n",
    "    output:\n",
    "    * location_df: spark df, containing location info\n",
    "    '''\n",
    "    location_pandas_df = pd.read_csv(path)\n",
    "    \n",
    "    location_spark_df = pandas_to_spark(sqlContext,location_pandas_df)\n",
    "    \n",
    "    location_spark_df = location_spark_df.withColumn(\"points\",struct(col(\"latitude\").cast(\"Float\"),col(\"longitude\").cast(\"Float\")))\n",
    "    # convert into spark df's time dtype\n",
    "    location_spark_df = location_spark_df.withColumn(\"datetime\", to_timestamp(location_spark_df.time, 'yyyy-MM-dd HH:mm:ss'))\n",
    "    return location_spark_df.select(['track_id','points','datetime'])\n",
    "\n",
    "@udf(\"string\")\n",
    "def geohash_w_time(struct_input):\n",
    "    # using geohash to put points to bins\n",
    "    encoded = gh.encode(struct_input[0],struct_input[1],precision=6)\n",
    "    return encoded\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=load_basic_location_data(path,sqlContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-------------------+\n",
      "|track_id|              points|           datetime|\n",
      "+--------+--------------------+-------------------+\n",
      "|       1|[-10.939342, -37....|2014-09-13 07:24:32|\n",
      "|       1|[-10.939342, -37....|2014-09-13 07:24:37|\n",
      "|       1|[-10.939324, -37....|2014-09-13 07:24:42|\n",
      "|       1|[-10.939211, -37....|2014-09-13 07:24:47|\n",
      "|       1|[-10.938939, -37....|2014-09-13 07:24:53|\n",
      "|       1|[-10.938543, -37....|2014-09-13 07:24:59|\n",
      "|       1|[-10.938346, -37....|2014-09-13 07:25:04|\n",
      "|       1|[-10.938449, -37....|2014-09-13 07:25:10|\n",
      "|       1|[-10.938666, -37....|2014-09-13 07:25:15|\n",
      "|       1|[-10.938986, -37....|2014-09-13 07:25:21|\n",
      "|       1|[-10.9393425, -37...|2014-09-13 07:25:27|\n",
      "|       1|[-10.939641, -37....|2014-09-13 07:25:32|\n",
      "|       1|[-10.9398575, -37...|2014-09-13 07:25:38|\n",
      "|       1|[-10.940077, -37....|2014-09-13 07:25:43|\n",
      "|       1|[-10.940389, -37....|2014-09-13 07:25:49|\n",
      "|       1|[-10.940746, -37....|2014-09-13 07:25:54|\n",
      "|       1|[-10.941179, -37....|2014-09-13 07:26:00|\n",
      "|       1|[-10.94148, -37.0...|2014-09-13 07:26:06|\n",
      "|       1|[-10.941532, -37....|2014-09-13 07:26:12|\n",
      "|       1|[-10.941532, -37....|2014-09-13 07:26:18|\n",
      "+--------+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "A.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|               time|squared_udf(points)|\n",
      "+-------------------+-------------------+\n",
      "|2014-09-13 07:24:32|             7nj9u8|\n",
      "|2014-09-13 07:24:37|             7nj9u8|\n",
      "|2014-09-13 07:24:42|             7nj9u8|\n",
      "|2014-09-13 07:24:47|             7nj9u8|\n",
      "|2014-09-13 07:24:53|             7nj9u8|\n",
      "|2014-09-13 07:24:59|             7nj9u8|\n",
      "|2014-09-13 07:25:04|             7nj9u8|\n",
      "|2014-09-13 07:25:10|             7nj9u8|\n",
      "|2014-09-13 07:25:15|             7nj9u8|\n",
      "|2014-09-13 07:25:21|             7nj9u8|\n",
      "|2014-09-13 07:25:27|             7nj9u8|\n",
      "|2014-09-13 07:25:32|             7nj9u8|\n",
      "|2014-09-13 07:25:38|             7nj9u8|\n",
      "|2014-09-13 07:25:43|             7nj9u8|\n",
      "|2014-09-13 07:25:49|             7nj9u8|\n",
      "|2014-09-13 07:25:54|             7nj9u8|\n",
      "|2014-09-13 07:26:00|             7nj9u8|\n",
      "|2014-09-13 07:26:06|             7nj9u8|\n",
      "|2014-09-13 07:26:12|             7nj9u8|\n",
      "|2014-09-13 07:26:18|             7nj9u8|\n",
      "+-------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "A.select('time',squared_udf('points')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.select(\"id\", squared_udf(\"id\").alias(\"id_squared\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-3698a8b9b7fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctions\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcollect_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msubstring\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mudf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "def row_processor_geohash_w_time(row,day_hours_list,night_hours_list):\n",
    "    '''\n",
    "    geohashing function to encode location points into\n",
    "    hashed str values, geohash precision set to 6\n",
    "    input: \n",
    "    * row: list of tuples\n",
    "    output:\n",
    "    * encoded_points: set of str\n",
    "    '''\n",
    "    #using set, to store unique points\n",
    "    daytime_locs = []\n",
    "    nighttime_locs = []\n",
    "    \n",
    "    for tup in row:\n",
    "        time = tup[0]\n",
    "        encoded = gh.encode(tup[1][0],tup[1][1],precision=6)\n",
    "        \n",
    "        if time[-2:] in day_hours_list:\n",
    "            daytime_locs.append(encoded)\n",
    "            \n",
    "        elif time[-2:] in night_hours_list:\n",
    "            nighttime_locs.append(encoded)\n",
    "        \n",
    "        else:\n",
    "            #not in special hours, ignore\n",
    "            continue\n",
    "        \n",
    "    daytime_dict = Counter(daytime_locs)\n",
    "    nighttime_dcit = Counter(nighttime_locs)\n",
    "    \n",
    "    daytime_points = list(map(list, daytime_dict.items()))\n",
    "    nighttime_points = list(map(list, nighttime_dcit.items()))\n",
    "\n",
    "    return daytime_points,nighttime_points\n",
    "\n",
    "def retrieve_major_stationary_points(row):\n",
    "    '''\n",
    "    row: tuple of lists\n",
    "    '''\n",
    "    daytime_stationary = row[0]\n",
    "    nighttime_stationary = row[1]\n",
    "    \n",
    "    #only select the most voted point\n",
    "    major_daytime_pt, major_nighttime_pt = 'none','none'\n",
    "    \n",
    "    if len(daytime_stationary) > 0:\n",
    "        #str values\n",
    "        major_daytime_pt = sorted(daytime_stationary, key=lambda x: x[1], reverse = True)[0][0]\n",
    "    if len(nighttime_stationary) > 0:\n",
    "        major_nighttime_pt = sorted(nighttime_stationary, key=lambda x: x[1], reverse = True)[0][0]\n",
    "    \n",
    "    return major_daytime_pt,major_nighttime_pt\n",
    "\n",
    "def generate_stationary_points(period,spark,day_hours_list,night_hours_list):\n",
    "    '''\n",
    "    based on the special hours defined\n",
    "    find daytime nighttime stay points\n",
    "    '''\n",
    "    period_agg_special_hours = agg_geohash_w_time(period,spark)\n",
    "    \n",
    "    period_agg_special_hours_pdf = period_agg_special_hours.toPandas()\n",
    "    \n",
    "    period_agg_special_hours_pdf['day_night_locations'] = period_agg_special_hours_pdf['mobility_gene_points_w_time'].apply(lambda x: row_processor_geohash_w_time(x,day_hours_list,night_hours_list))\n",
    "    \n",
    "    period_agg_special_hours_pdf['major_day_night_location'] = period_agg_special_hours_pdf['day_night_locations'].apply(lambda x:retrieve_major_stationary_points(x))\n",
    "    \n",
    "    return period_agg_special_hours_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
